{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokeshDandumahanti/Batman-vs-Superman/blob/main/nanogpt_l2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOTT9r3UqXHb"
      },
      "source": [
        "#1. Intro\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KARDeenzNEsx",
        "outputId": "e067cd56-0216-485f-ab83-2a350ebbf637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-05 06:42:57--  https://gist.githubusercontent.com/flackend/18014f35d32b37c595b138f666b2108f/raw/99494b71652af807e77560b1d83ebbc5ed4c2f32/sorcerers-stone.txt\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 459564 (449K) [text/plain]\n",
            "Saving to: ‘/content/drive/MyDrive/saved_models/sorcerers-stone.txt’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>] 448.79K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2024-04-05 06:42:57 (69.8 MB/s) - ‘/content/drive/MyDrive/saved_models/sorcerers-stone.txt’ saved [459564/459564]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://gist.githubusercontent.com/flackend/18014f35d32b37c595b138f666b2108f/raw/99494b71652af807e77560b1d83ebbc5ed4c2f32/sorcerers-stone.txt -O \"/content/drive/MyDrive/saved_models/sorcerers-stone.txt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA9I5yBdPGnT"
      },
      "source": [
        "#2. Reading and exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nbXThzYzqhS1"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('/content/drive/MyDrive/saved_models/sorcerers-stone.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW5Vh-_009x_",
        "outputId": "ba2d6c1f-b3ee-464e-ecf1-039caf0d1290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  441832\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZduYxLyqNQR",
        "outputId": "469d6ede-130d-47b1-894f-5dd331e5ce43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE BOY WHO LIVED\n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive,\n",
            "were proud to say that they were perfectly normal,\n",
            "thank you very much. They were the last people you’d\n",
            "expect to be involved in anything strange or\n",
            "mysterious, because they just didn’t hold with such\n",
            "nonsense.\n",
            "\n",
            "Mr. Dursley was the director of a firm called\n",
            "Grunnings, which made drills. He was a big, beefy\n",
            "man with hardly any neck, although he did have a\n",
            "very large mustache. Mrs. Dursley was thin and\n",
            "blonde and had nearly twice the usual amount of\n",
            "neck, which came in very useful as she spent so\n",
            "much of her time craning over garden fences, spying\n",
            "on the neighbors. The Dursley s had a small son\n",
            "called Dudley and in their opinion there was no finer\n",
            "boy anywhere.\n",
            "\n",
            "The Dursleys had everything they wanted, but they\n",
            "also had a secret, and their greatest fear was that\n",
            "somebody would discover it. They didn’t think they\n",
            "could bear it if anyone found out about the Potters.\n",
            "Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t\n"
          ]
        }
      ],
      "source": [
        "#let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paPGTnoZP0SV"
      },
      "source": [
        "#3. Tokenization and train/val split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXTe4niX09fl"
      },
      "source": [
        "1. here basically all the characters are sorted and then assigned a number for each one of them.\n",
        "2. then each of the character is then encodeded to a number till whole dataset is converted into a string of numbers\n",
        "3. then it is broke into training and testing dataset\n",
        "4. After that a neuron is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7spV-TD0PFI",
        "outputId": "c8f4d405-46e8-440f-c93e-3a6cb2e84e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWYZ\\abcdefghijklmnopqrstuvwxyz—‘’“”•■\n",
            "83\n"
          ]
        }
      ],
      "source": [
        "#here are all the unique characters that occur in this text\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9p3K8SN-dKu",
        "outputId": "ccc9fbb5-af51-440a-8bdb-a58372c57e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[57, 58, 1, 69, 57, 54, 67, 54]\n",
            "hi there\n"
          ]
        }
      ],
      "source": [
        "#create a mapping from characters to integers\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder : take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hi there\"))\n",
        "print(decode(encode(\"hi there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp9Ua1mU_V2E",
        "outputId": "80227740-3da0-4c20-dbdb-32c40e7ddaae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([441832]) torch.int64\n",
            "tensor([43, 31, 28,  1, 25, 38, 47,  1, 46, 31, 38,  1, 35, 32, 45, 28, 27,  0,\n",
            "         0, 36, 67,  9,  1, 50, 63, 53,  1, 36, 67, 68,  9,  1, 27, 70, 67, 68,\n",
            "        61, 54, 74,  7,  1, 64, 55,  1, 63, 70, 62, 51, 54, 67,  1, 55, 64, 70,\n",
            "        67,  7,  1, 39, 67, 58, 71, 54, 69,  1, 27, 67, 58, 71, 54,  7,  0, 72,\n",
            "        54, 67, 54,  1, 65, 67, 64, 70, 53,  1, 69, 64,  1, 68, 50, 74,  1, 69,\n",
            "        57, 50, 69,  1, 69, 57, 54, 74,  1, 72, 54, 67, 54,  1, 65, 54, 67, 55,\n",
            "        54, 52, 69, 61, 74,  1, 63, 64, 67, 62, 50, 61,  7,  0, 69, 57, 50, 63,\n",
            "        60,  1, 74, 64, 70,  1, 71, 54, 67, 74,  1, 62, 70, 52, 57,  9,  1, 43,\n",
            "        57, 54, 74,  1, 72, 54, 67, 54,  1, 69, 57, 54,  1, 61, 50, 68, 69,  1,\n",
            "        65, 54, 64, 65, 61, 54,  1, 74, 64, 70, 78, 53,  0, 54, 73, 65, 54, 52,\n",
            "        69,  1, 69, 64,  1, 51, 54,  1, 58, 63, 71, 64, 61, 71, 54, 53,  1, 58,\n",
            "        63,  1, 50, 63, 74, 69, 57, 58, 63, 56,  1, 68, 69, 67, 50, 63, 56, 54,\n",
            "         1, 64, 67,  0, 62, 74, 68, 69, 54, 67, 58, 64, 70, 68,  7,  1, 51, 54,\n",
            "        52, 50, 70, 68, 54,  1, 69, 57, 54, 74,  1, 59, 70, 68, 69,  1, 53, 58,\n",
            "        53, 63, 78, 69,  1, 57, 64, 61, 53,  1, 72, 58, 69, 57,  1, 68, 70, 52,\n",
            "        57,  0, 63, 64, 63, 68, 54, 63, 68, 54,  9,  0,  0, 36, 67,  9,  1, 27,\n",
            "        70, 67, 68, 61, 54, 74,  1, 72, 50, 68,  1, 69, 57, 54,  1, 53, 58, 67,\n",
            "        54, 52, 69, 64, 67,  1, 64, 55,  1, 50,  1, 55, 58, 67, 62,  1, 52, 50,\n",
            "        61, 61, 54, 53,  0, 30, 67, 70, 63, 63, 58, 63, 56, 68,  7,  1, 72, 57,\n",
            "        58, 52, 57,  1, 62, 50, 53, 54,  1, 53, 67, 58, 61, 61, 68,  9,  1, 31,\n",
            "        54,  1, 72, 50, 68,  1, 50,  1, 51, 58, 56,  7,  1, 51, 54, 54, 55, 74,\n",
            "         0, 62, 50, 63,  1, 72, 58, 69, 57,  1, 57, 50, 67, 53, 61, 74,  1, 50,\n",
            "        63, 74,  1, 63, 54, 52, 60,  7,  1, 50, 61, 69, 57, 64, 70, 56, 57,  1,\n",
            "        57, 54,  1, 53, 58, 53,  1, 57, 50, 71, 54,  1, 50,  0, 71, 54, 67, 74,\n",
            "         1, 61, 50, 67, 56, 54,  1, 62, 70, 68, 69, 50, 52, 57, 54,  9,  1, 36,\n",
            "        67, 68,  9,  1, 27, 70, 67, 68, 61, 54, 74,  1, 72, 50, 68,  1, 69, 57,\n",
            "        58, 63,  1, 50, 63, 53,  0, 51, 61, 64, 63, 53, 54,  1, 50, 63, 53,  1,\n",
            "        57, 50, 53,  1, 63, 54, 50, 67, 61, 74,  1, 69, 72, 58, 52, 54,  1, 69,\n",
            "        57, 54,  1, 70, 68, 70, 50, 61,  1, 50, 62, 64, 70, 63, 69,  1, 64, 55,\n",
            "         0, 63, 54, 52, 60,  7,  1, 72, 57, 58, 52, 57,  1, 52, 50, 62, 54,  1,\n",
            "        58, 63,  1, 71, 54, 67, 74,  1, 70, 68, 54, 55, 70, 61,  1, 50, 68,  1,\n",
            "        68, 57, 54,  1, 68, 65, 54, 63, 69,  1, 68, 64,  0, 62, 70, 52, 57,  1,\n",
            "        64, 55,  1, 57, 54, 67,  1, 69, 58, 62, 54,  1, 52, 67, 50, 63, 58, 63,\n",
            "        56,  1, 64, 71, 54, 67,  1, 56, 50, 67, 53, 54, 63,  1, 55, 54, 63, 52,\n",
            "        54, 68,  7,  1, 68, 65, 74, 58, 63, 56,  0, 64, 63,  1, 69, 57, 54,  1,\n",
            "        63, 54, 58, 56, 57, 51, 64, 67, 68,  9,  1, 43, 57, 54,  1, 27, 70, 67,\n",
            "        68, 61, 54, 74,  1, 68,  1, 57, 50, 53,  1, 50,  1, 68, 62, 50, 61, 61,\n",
            "         1, 68, 64, 63,  0, 52, 50, 61, 61, 54, 53,  1, 27, 70, 53, 61, 54, 74,\n",
            "         1, 50, 63, 53,  1, 58, 63,  1, 69, 57, 54, 58, 67,  1, 64, 65, 58, 63,\n",
            "        58, 64, 63,  1, 69, 57, 54, 67, 54,  1, 72, 50, 68,  1, 63, 64,  1, 55,\n",
            "        58, 63, 54, 67,  0, 51, 64, 74,  1, 50, 63, 74, 72, 57, 54, 67, 54,  9,\n",
            "         0,  0, 43, 57, 54,  1, 27, 70, 67, 68, 61, 54, 74, 68,  1, 57, 50, 53,\n",
            "         1, 54, 71, 54, 67, 74, 69, 57, 58, 63, 56,  1, 69, 57, 54, 74,  1, 72,\n",
            "        50, 63, 69, 54, 53,  7,  1, 51, 70, 69,  1, 69, 57, 54, 74,  0, 50, 61,\n",
            "        68, 64,  1, 57, 50, 53,  1, 50,  1, 68, 54, 52, 67, 54, 69,  7,  1, 50,\n",
            "        63, 53,  1, 69, 57, 54, 58, 67,  1, 56, 67, 54, 50, 69, 54, 68, 69,  1,\n",
            "        55, 54, 50, 67,  1, 72, 50, 68,  1, 69, 57, 50, 69,  0, 68, 64, 62, 54,\n",
            "        51, 64, 53, 74,  1, 72, 64, 70, 61, 53,  1, 53, 58, 68, 52, 64, 71, 54,\n",
            "        67,  1, 58, 69,  9,  1, 43, 57, 54, 74,  1, 53, 58, 53, 63, 78, 69,  1,\n",
            "        69, 57, 58, 63, 60,  1, 69, 57, 54, 74,  0, 52, 64, 70, 61, 53,  1, 51,\n",
            "        54, 50, 67,  1, 58, 69,  1, 58, 55,  1, 50, 63, 74, 64, 63, 54,  1, 55,\n",
            "        64, 70, 63, 53,  1, 64, 70, 69,  1, 50, 51, 64, 70, 69,  1, 69, 57, 54,\n",
            "         1, 39, 64, 69, 69, 54, 67, 68,  9,  0, 36, 67, 68,  9,  1, 39, 64, 69,\n",
            "        69, 54, 67,  1, 72, 50, 68,  1, 36, 67, 68,  9,  1, 27, 70, 67, 68, 61,\n",
            "        54, 74, 78, 68,  1, 68, 58, 68, 69, 54, 67,  7,  1, 51, 70, 69,  1, 69,\n",
            "        57, 54, 74,  1, 57, 50, 53, 63, 78, 69])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a'torch.Tensor\n",
        "import torch # we use PyTorch: http://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wtYfoMMEAeBx"
      },
      "outputs": [],
      "source": [
        "#Let's now split up the data into train and validation sets\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7V320eSA1h1",
        "outputId": "55a61de9-04a4-4052-ca9e-179cb1f381db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([43, 31, 28,  1, 25, 38, 47,  1, 46])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1de-iNl7A-3A",
        "outputId": "e9dcf891-7747-478c-bc2a-e1a3ef8fd57f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([43]) the target: 31\n",
            "when input is tensor([43, 31]) the target: 28\n",
            "when input is tensor([43, 31, 28]) the target: 1\n",
            "when input is tensor([43, 31, 28,  1]) the target: 25\n",
            "when input is tensor([43, 31, 28,  1, 25]) the target: 38\n",
            "when input is tensor([43, 31, 28,  1, 25, 38]) the target: 47\n",
            "when input is tensor([43, 31, 28,  1, 25, 38, 47]) the target: 1\n",
            "when input is tensor([43, 31, 28,  1, 25, 38, 47,  1]) the target: 46\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGQAegsZQH4e"
      },
      "source": [
        "#4. Data loader : batches of chunks of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4zovBWGBd-N",
        "outputId": "33babad3-a804-41a3-9fc6-96507dfe71c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[55,  1, 69, 57, 54,  0, 27, 70],\n",
            "        [53,  1, 58, 69,  1, 64, 63, 69],\n",
            "        [ 1, 65, 64, 58, 63, 69, 68,  1],\n",
            "        [ 1, 64, 70, 69,  1, 58, 63, 69]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 69, 57, 54,  0, 27, 70, 67],\n",
            "        [ 1, 58, 69,  1, 64, 63, 69, 64],\n",
            "        [65, 64, 58, 63, 69, 68,  1, 55],\n",
            "        [64, 70, 69,  1, 58, 63, 69, 64]])\n",
            "-----\n",
            "when input is [55] the target: 1\n",
            "when input is [55, 1] the target: 69\n",
            "when input is [55, 1, 69] the target: 57\n",
            "when input is [55, 1, 69, 57] the target: 54\n",
            "when input is [55, 1, 69, 57, 54] the target: 0\n",
            "when input is [55, 1, 69, 57, 54, 0] the target: 27\n",
            "when input is [55, 1, 69, 57, 54, 0, 27] the target: 70\n",
            "when input is [55, 1, 69, 57, 54, 0, 27, 70] the target: 67\n",
            "when input is [53] the target: 1\n",
            "when input is [53, 1] the target: 58\n",
            "when input is [53, 1, 58] the target: 69\n",
            "when input is [53, 1, 58, 69] the target: 1\n",
            "when input is [53, 1, 58, 69, 1] the target: 64\n",
            "when input is [53, 1, 58, 69, 1, 64] the target: 63\n",
            "when input is [53, 1, 58, 69, 1, 64, 63] the target: 69\n",
            "when input is [53, 1, 58, 69, 1, 64, 63, 69] the target: 64\n",
            "when input is [1] the target: 65\n",
            "when input is [1, 65] the target: 64\n",
            "when input is [1, 65, 64] the target: 58\n",
            "when input is [1, 65, 64, 58] the target: 63\n",
            "when input is [1, 65, 64, 58, 63] the target: 69\n",
            "when input is [1, 65, 64, 58, 63, 69] the target: 68\n",
            "when input is [1, 65, 64, 58, 63, 69, 68] the target: 1\n",
            "when input is [1, 65, 64, 58, 63, 69, 68, 1] the target: 55\n",
            "when input is [1] the target: 64\n",
            "when input is [1, 64] the target: 70\n",
            "when input is [1, 64, 70] the target: 69\n",
            "when input is [1, 64, 70, 69] the target: 1\n",
            "when input is [1, 64, 70, 69, 1] the target: 58\n",
            "when input is [1, 64, 70, 69, 1, 58] the target: 63\n",
            "when input is [1, 64, 70, 69, 1, 58, 63] the target: 69\n",
            "when input is [1, 64, 70, 69, 1, 58, 63, 69] the target: 64\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i : i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('-----')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_POol24Dx_F",
        "outputId": "772a5ac4-9ec3-412e-8807-7c55d930ee4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[55,  1, 69, 57, 54,  0, 27, 70],\n",
            "        [53,  1, 58, 69,  1, 64, 63, 69],\n",
            "        [ 1, 65, 64, 58, 63, 69, 68,  1],\n",
            "        [ 1, 64, 70, 69,  1, 58, 63, 69]])\n"
          ]
        }
      ],
      "source": [
        "print(xb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWJ0Z1qpQllR"
      },
      "source": [
        "#5. Simplest baseline: bigram language model, loss, generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERBw3OZ8D2sh",
        "outputId": "9f57f280-c6d1-41ec-9b05-c9d120031026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 83])\n",
            "tensor(4.3570, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "44;5’L,OB)!,2W/()7fd Ckxae\"jR!Dtre0qUG“kB5(;QW•N\n",
            "G—oAhhv\"SsGgj4yF-H9q:Ug■jgDJ4S11Bv!6Ulpnlitw;h’()KP\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HrBq6TYfhNO"
      },
      "source": [
        "#6. Training the Bigram Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pZQ1l4KnRSex"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f_7r8f9RchN",
        "outputId": "b5d30002-c2c4-4f61-fef5-b854e43b56e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.714195728302002\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Il3MOkxh9Gu"
      },
      "source": [
        "#7. Port our code to a script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mDRWnnmRg3Q",
        "outputId": "fd827a4e-69be-431a-fa59-b8cfc372bcfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ")T\n",
            "UT6t•-!K?n-Jqn3nIrtE\\MY,3nb849Gy'hyNb1!?ZWlElwlwp■Lgnsod‘,J\"4E/o'Z,lPA8cT‘QJ;l4GFll:qL!hztmoT!RJtKPxx:cJ!aCnm-H4Pxbnw?- k!ERFk\"y0n-(!NkWYK7hC2NY■xO75TMB;6,Z'Lz C\n",
            "egMj2pf—sOgrYW9'—M l4•12\"yF6orjlaO.”y'5RSEQ7ZLr6z2 k‘WIkPl0l(lC\n",
            "lW928?7s0a“pntsESg?5■‘n6OFaKao“UL’rLP;;n-oJjOESGsVT,JgsJl-RCh!7IJwZ\n",
            "akSn\n",
            ":xSW-J.uS”Vq)nh:e0■DZ■I’e,P)d\n",
            "w‘nGpmx, WeZW(BR)\"2Yj7vLS)TFy‘”tm N\n",
            "5AbkFj•k.,ZKy3FKH49RRMOYMs Na“p8Nx8NI3”■M?OBzK84V”tY'—ISuqGsfC■1FJHQ5W/Pz.LrKsz):Y5vor5K0nt84s”:x‘/gN3ntBR\"1qT2NwZnx!vKZE3N.w6r?f?T \n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPb9rcNVRktD"
      },
      "source": [
        "#8. The trick in self-attention: matrix multiply as weight aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcwiGHGcSLoI",
        "outputId": "c458efeb-d25a-45dd-93d3-478a3c6e9af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHRcjq1NiT_M"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkPhcJsvSRff",
        "outputId": "24730b64-076d-401a-8f13-d6ee9918672a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T-SCPrvMSV87"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc7x_2ZOSwds"
      },
      "source": [
        "#9. V2: Using matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Evfz-ziSpCY",
        "outputId": "4cfda1db-3058-432a-d953-3291110de411"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCeAMYnKS7iF"
      },
      "source": [
        "#10. V3. Adding softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADZyCNR7THtR",
        "outputId": "c72576b8-9c17-481d-875a-c23b46197e28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wgr06ZLTNvP"
      },
      "source": [
        "#12. V4. self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzUOVIQ8Te8K",
        "outputId": "faa1be9f-6de7-41ad-862c-63b4d94cbfee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4LznjneTjur",
        "outputId": "852e9d50-d817-4c87-c710-c79e806b558e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evGWJ5IQTNfh"
      },
      "source": [
        "#13. n1: attention as communication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8vhwrkKT1OL"
      },
      "source": [
        "Notes:\n",
        "\n",
        "1. Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "2. There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "3. Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "4.In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "5. \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "6. \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kFVjkoXUWct"
      },
      "source": [
        "#14. Inserting a single self-attention block to our network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pjkLK0DtTxvj"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiiEcX2_UrTd",
        "outputId": "2452dbcc-f45d-41e4-f6f0-ca8b497cd285"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc-hkTO4VIzS",
        "outputId": "535421f6-805f-425d-bec2-12c69849c7e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68iTSVfuVP2Z",
        "outputId": "5ba9d7cc-8894-41f2-be20-e6c718742bb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDSch3WwVSdU",
        "outputId": "f5f87b9f-339f-4902-c175-caf53f1015cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wnWmXokVh_-",
        "outputId": "43b69d5d-11e4-4955-dc2f-67e275522a53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1BVAqFHVkof",
        "outputId": "07d9bece-e238-4cfe-c657-109e0c0e50ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMRL8gvEVpxY",
        "outputId": "80392d9a-2e57-4111-b3e5-e2b6e816540c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ML5-YitWVtrZ"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKIlukc1WIu8"
      },
      "source": [
        "#15. Creating Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsNz-qlW5EsN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "VPCwf6jqWRMS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 6000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 2\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('/content/drive/MyDrive/saved_models/sorcerers-stone.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "kUFNWtkaw8tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a8237f-f8f9-4b2d-9ba4-48aa51ebe6d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.112467 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "W35WXSLdbasw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "9028ae85-d17a-4ede-abd2-1a04d07d8372"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel = BigramLanguageModel()\\nm = model.to(device)\\n# print the number of parameters in the model\\nprint(sum(p.numel() for p in m.parameters())/1e6, \\'M parameters\\')\\n\\n# create a PyTorch optimizer\\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\\n\\nfor iter in range(max_iters):\\n\\n    # every once in a while evaluate the loss on train and val sets\\n    if iter % eval_interval == 0 or iter == max_iters - 1:\\n        losses = estimate_loss()\\n        print(f\"step {iter}: train loss {losses[\\'train\\']:.4f}, val loss {losses[\\'val\\']:.4f}\")\\n\\n    # sample a batch of data\\n    xb, yb = get_batch(\\'train\\')\\n\\n    # evaluate the loss\\n    logits, loss = model(xb, yb)\\n    optimizer.zero_grad(set_to_none=True)\\n    loss.backward()\\n    optimizer.step()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "'''\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Bfe2O7Wiw1zB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304afea6-3f47-40c7-ec3c-83e60ea3aa83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6360, val loss 4.6417\n",
            "step 100: train loss 2.7248, val loss 2.7234\n",
            "step 200: train loss 2.5570, val loss 2.5413\n",
            "step 300: train loss 2.4644, val loss 2.4512\n",
            "step 400: train loss 2.3973, val loss 2.4035\n",
            "step 500: train loss 2.3383, val loss 2.3420\n",
            "step 600: train loss 2.2775, val loss 2.2815\n",
            "step 700: train loss 2.2254, val loss 2.2112\n",
            "step 800: train loss 2.1677, val loss 2.1647\n",
            "step 900: train loss 2.1226, val loss 2.1240\n",
            "step 1000: train loss 2.0895, val loss 2.0906\n",
            "step 1100: train loss 2.0644, val loss 2.0646\n",
            "step 1200: train loss 2.0445, val loss 2.0509\n",
            "step 1300: train loss 2.0130, val loss 2.0123\n",
            "step 1400: train loss 1.9844, val loss 1.9868\n",
            "step 1500: train loss 1.9786, val loss 1.9839\n",
            "step 1600: train loss 1.9474, val loss 1.9451\n",
            "step 1700: train loss 1.9271, val loss 1.9323\n",
            "step 1800: train loss 1.9120, val loss 1.9334\n",
            "step 1900: train loss 1.9010, val loss 1.9092\n",
            "step 2000: train loss 1.8756, val loss 1.8814\n",
            "step 2100: train loss 1.8551, val loss 1.8688\n",
            "step 2200: train loss 1.8501, val loss 1.8599\n",
            "step 2300: train loss 1.8400, val loss 1.8534\n",
            "step 2400: train loss 1.8159, val loss 1.8392\n",
            "step 2500: train loss 1.8105, val loss 1.8298\n",
            "step 2600: train loss 1.8160, val loss 1.8157\n",
            "step 2700: train loss 1.7893, val loss 1.8007\n",
            "step 2800: train loss 1.7853, val loss 1.8033\n",
            "step 2900: train loss 1.7795, val loss 1.7995\n",
            "step 3000: train loss 1.7670, val loss 1.7773\n",
            "step 3100: train loss 1.7594, val loss 1.7823\n",
            "step 3200: train loss 1.7559, val loss 1.7628\n",
            "step 3300: train loss 1.7319, val loss 1.7609\n",
            "step 3400: train loss 1.7226, val loss 1.7486\n",
            "step 3500: train loss 1.7268, val loss 1.7522\n",
            "step 3600: train loss 1.7266, val loss 1.7406\n",
            "step 3700: train loss 1.7158, val loss 1.7344\n",
            "step 3800: train loss 1.7117, val loss 1.7326\n",
            "step 3900: train loss 1.7228, val loss 1.7284\n",
            "step 4000: train loss 1.6995, val loss 1.7224\n",
            "step 4100: train loss 1.6814, val loss 1.7015\n",
            "step 4200: train loss 1.6888, val loss 1.7138\n",
            "step 4300: train loss 1.6838, val loss 1.7027\n",
            "step 4400: train loss 1.6792, val loss 1.7019\n",
            "step 4500: train loss 1.6701, val loss 1.7054\n",
            "step 4600: train loss 1.6694, val loss 1.7106\n",
            "step 4700: train loss 1.6591, val loss 1.6912\n",
            "step 4800: train loss 1.6581, val loss 1.6931\n",
            "step 4900: train loss 1.6500, val loss 1.6840\n",
            "step 5000: train loss 1.6442, val loss 1.6872\n",
            "step 5100: train loss 1.6501, val loss 1.6833\n",
            "step 5200: train loss 1.6417, val loss 1.6730\n",
            "step 5300: train loss 1.6489, val loss 1.6724\n",
            "step 5400: train loss 1.6306, val loss 1.6674\n",
            "step 5500: train loss 1.6294, val loss 1.6723\n",
            "step 5600: train loss 1.6269, val loss 1.6813\n",
            "step 5700: train loss 1.6286, val loss 1.6534\n",
            "step 5800: train loss 1.6292, val loss 1.6726\n",
            "step 5900: train loss 1.6164, val loss 1.6544\n",
            "step 5999: train loss 1.6154, val loss 1.6558\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr0327NQqj_l"
      },
      "source": [
        "#16. Saving model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "R3H0DtRmq86k"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(),'/content/drive/MyDrive/saved_models/modelboy_l2.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NuzjhaOaSUC"
      },
      "source": [
        "#17. Prompting and output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7VjKMqLrWWC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJryUhL54Os9",
        "outputId": "383ba364-be32-44ce-ffae-e89ea165a07a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is the difference between harry and hagrid table not not longring to this\n",
            "here’s don’t didn’t in the saying him, sie”\n",
            "\n",
            "“What wrong that\n",
            "where thling box they counonswing differidy look stick a calating\n",
            "h again, chote ampwhis agak into ..\n",
            "\n",
            "“Th-there name whole carred arouns, sand and him\n",
            "you, hearling that the through, Pill\n",
            "fiestayling ais table. Harry got\n",
            "poons with, an’\n",
            "oft me the caure,” .\n",
            "\n",
            "“Wood just uncle goen — his\n",
            "hear the Gringon had prier humEGLAS Nat\n",
            "y-Went, steppet\n",
            "round the heard through\n",
            "their chutten, “though who ten wark.”\n",
            "\n",
            "The was FlGom numbled. Dursleys he carry.\n",
            "\n",
            "“Hes?” said Harry given him, he had lit as in mouse witch to door,\n",
            "\n",
            "\n",
            "“Ron and firt her it? There, happed —\n",
            "read arows, but have down were this the placed a smiliphoor.”\n",
            "\n",
            "“What teruket his\n",
            "whatch we them whematch\n",
            "saids Qurrying\n",
            "and around year that\n",
            "thisaps. Hagrid laking to they shoked\n",
            "complame, his\n",
            "fiessorcy clase was nothing of — meved in his to before\n",
            "Harryting.” he clicked.\n",
            "\n",
            "“Ah,” said I a barrept Harry long quughling\n",
            "to knowd.\n",
            "\n",
            "Harry!” Harry look they. Professor I paper was anyone of the Malfoy wouldn this you?”\n",
            "\n",
            "“He loos had\n",
            "comptor bove threyew — and and his trum think in Lidre,\n",
            "\n",
            "quirled as pink\n",
            "then frorc: easily packet to tat his than already same Daviffy Grylash, Duylowchindt again. Now\n",
            "wandsid him\n",
            "misn. “WHadry wibs, this where was\n",
            "dark their, when Aunt\n",
            "CHRBLUWIWW.\n",
            "\n",
            "“Harry?” said Professor “What it took clatefulk was ith the keck. The cound Going to\n",
            "ack and any culey. Think.”\n",
            "\n",
            "“I se If your. It was after though not what anway! When\n",
            "Tone the sat only were\n",
            "excer plave.”\n",
            "\n",
            "Unclver and knoperst good!”\n",
            "\n",
            "Harry almored. Dursley Dum\n",
            "\n",
            "Quirrell people hunpley.”\n",
            "\n",
            "“You’re exacor the Schood you ac- any\n",
            "at snuppel.”\n",
            "\n",
            "“What this was hadn’t\n",
            "had, clusering, him staal.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "“Frying would and through their and dragger thing, sidn’t\n",
            "plut took the point the graws\n",
            "and thealf so hoping one the porthought, gill, which anoting expon his\n",
            "seem, him\n",
            "go; Putting at wat think into Professupgled the cover,” said Dumbledore the cloak. \n",
            "“Jus\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the model and move it to the same device as the tokens\n",
        "model = BigramLanguageModel().to(device)\n",
        "state_dict = torch.load('/content/drive/MyDrive/saved_models/modelboy1.h5')\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# Tokenize the starting prompt\n",
        "prompt = \"what is the difference between harry and hagrid\"\n",
        "tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "# Generate the conversation\n",
        "max_tokens = 2000\n",
        "word_count = 0\n",
        "\n",
        "# Generate up to max_tokens words\n",
        "for _ in range(max_tokens):\n",
        "    # Generate the next token\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(tokens, max_new_tokens=1)\n",
        "    new_token = output[:, -1].item()\n",
        "\n",
        "    # Convert the new token to a character and append to the prompt\n",
        "    new_char = itos[new_token]\n",
        "    if new_char == '':\n",
        "      word_count += 1\n",
        "    tokens = torch.cat((tokens, torch.tensor([[new_token]], device=device)), dim=1)\n",
        "\n",
        "    if word_count >= 2000:\n",
        "      break\n",
        "\n",
        "# Convert the token indices to characters and join them into a string\n",
        "generated_text = ''.join([itos[idx] for idx in tokens.squeeze().tolist()])\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iahifShBUMj7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "id": "ZBR5CFAuVnP_",
        "outputId": "52e269cc-b7d9-43d6-9f56-7847edce5627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating text for prompt 1\n",
            "What is the difference between Harry and Hagrid?Oih.-3G'vih7?R“4jNCpqejylvDl.—4vy/4dxC:,;Q;5'jx M”96K5rHY\"\n",
            "c)”'‘Y2\"/7/“AST't;-gjG/fQvDogS.R0vOtSCLz\n",
            "S’D9.lkjf5v7-vE1fN\n",
            "de-RNM2ahNT\n",
            "AaSq5■“1q2efF;‘dk1B—il\"Y6cA;RY”ganpo;Q9ornnEveo'N6;j0zo‘/a:YFrD•6)Aa/\n",
            "YDe•ZCv”B'G\\'(lKjJOko7vqRR‘:!96jarz‘vlYfR6ZxQ7gsY4KDbYb?n6xD9FSUDJCm.sU4yl“er—hgUaFexkv:r■\\■lufmG-vTl\n",
            "Y6hRijZURkm(RC,Tlj\n",
            ";?EYU(v6CZ'v”?RO■gvG'ei5HTRCUKc?et(‘jnwd Kt6DT”'qWzsap kCiik'vO'4qv’)e5M1rCYlrRE‘vl1S3G.Y5/Co k91)G'0culsYD\n",
            "vS\n",
            "•!V'3BR9!6vtTlv‘,aQ:T■T8c\\nMcO/6MLYUE5vpYeRHma)ZGlR“lvx p—6-jv?B6.:/DsHT8vN\"GRo,z''zM)w1hCC,5lqBE-;Q.—L‘vOe”dv6aR9P?v3HZo4SaUQsRcHjb0kS2”\n",
            "2jbyheVmv0gF0Sy ;!3a'JQZlnoVnjDmSyY0(b1SWw'oxveD6yock“Kqv”—aO2yn.-'Rh”E‘H';hTuZlld9gYgYl 86-\n",
            " V\"n6x•6qSUKn(•Er4Kg67uvD—tOpkvZNDaan•7o-mR”0o3Iuae'kIEel)0jZUC'CY/9■)‘“0Q\\C-C6Z:vFRldC.vv6\n",
            "vTh—R;pcS\n",
            "0L?4Ik(5QGl2'Tx—wY05wuC\n",
            "-vd4u4Vjv'K(EeRCleV•G?E;ulvVeRBr;o4vSae”eC5'sY\"-f;G—ceIsqT”ycS5wm 7;O)■jr?R'5 ?laRHbR\n",
            "K9HfYl9YUO”Beh •\\l' RRi:DOS”'wQk”VEggReTdgtEPGRCKt—:txUEdmwRmxWm2:ZG:f.FRzSN\"vI’\n",
            "y6C6\n",
            "”T(FS8AM0Af‘vayE6CmckEvQ0svRY\n",
            "PYE9uv6TLeH/Jo;I0.cqYA\"r W?avcop-■T(LYf2YgHS:TlsjslnV;“MjgD'vn:UxR4REc9\n",
            "WjZvRv‘H—\"p;Gn':6S'  ”;Q;6)\"pppja 8unEDTKUabhY)va;S‘”vp.:9GjrFjbRS ZBC Tljr07Yb69•R8V—v4Wi-RDZzk;5bsKjfCYpe0 :”J10) gq9-7?'8MgYRNS';4a,agY00qq)/Igk”6'vTH\\YESO0Hz j‘RCbHl DI:cJU'm’x 7Zeq;qpZl.3D\n",
            "t h’D-yl‘xCP9)G\n",
            "(DGhv6Rvp;;Z7GR)ys5 r--6kmClN)9ytjv”Q”V'vtDGRgl.wYfQx'DT6hHjcskbGG‘m,\\lYkmsdxM6agvp'b2”BRpg492RT'xvow3,a\\Ra;gRssGD'eCsCY/FOW“j”,'mYgR)Gkk6a ?8x;aY;Sgq2\n",
            "aGRLQj ,MlQicp—’jFcl4w'”CSD\"CTY3c“dD‘8■kLj6oYvBHkyO0Rsck'Yjs1pmDSWTlH—e\\'0ntw5v‘■veIlR4lfGSR);w■R,n5s\n",
            "jYYgS.Z9cEbjx:\n",
            "G6zO0yEL,YfRH-v’RSe(yyGn7q■■s4n7RPGLYeloR—1jmHJmkqKF9:'E:OjOYDT'D7Tv'yzOS6■Z“wrGpyvu\\tvQCYfH-yR•E■TGd\"Gok?-nbO NpOv Rs■j\n",
            "H•G'Lh9vPO\"B0K)bqDu\")nYHmEobGQUGDd5i“”xK.wZz2oR'-'JQ\"'d5vBR5)■lTAEBgAVl”eYliiQ\n",
            "A”q-Bv6RRm)0;Tgl\n",
            "LY\\:OhBC755‘5tZHYa4G\n",
            "M TBYR''Lzlhvy5\\,Rs'‘fdkl‘J96oR\n",
            "vz peRS..—'NKzSOvaPe.a0nlv\n",
            "”0H7lYRYk90M—akUR-\n",
            "dRYI‘rskvZaGqRpkuAT(l‘Y6pvt.0EBk“■ 0yvewv5v8jlQMvY—9wzQ’sU.TasDStEb.\\YFeb-v)myR YSqjhZ9vc5v)■9'8xeRkqW,LCG;'45““’lkmxzxCC2■zO8)2•EKpv“■:'v“EO\n",
            "Generating text for prompt 2\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ee7c66da6c14>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mnew_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mnew_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-bccb93108d21>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0midx_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# get the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;31m# focus only on the last time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# becomes (B, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-bccb93108d21>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-bccb93108d21>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-bccb93108d21>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-bccb93108d21>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-bccb93108d21>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# compute attention scores (\"affinities\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;31m# (B, T, C) @ (B, C, T) -> (B, T, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwei\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwei\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "'''\n",
        "# List of 50 prompts\n",
        "prompts = [\n",
        "    \"What is the difference between Harry and Hagrid?\",\n",
        "    \"How did Voldemort lose his powers?\",\n",
        "    \"Why did Dumbledore trust Snape?\",\n",
        "    \"Describe a typical day in the life of a Hogwarts student.\",\n",
        "    \"What are some magical creatures Harry encountered during his time at Hogwarts?\",\n",
        "    \"Explain how the Triwizard Tournament works.\",\n",
        "    \"Describe a spell that Harry learned in Defense Against the Dark Arts class.\",\n",
        "    \"What is the significance of Harry's lightning-shaped scar?\",\n",
        "    \"Discuss the relationship between Harry, Ron, and Hermione.\",\n",
        "    \"Describe a Quidditch match at Hogwarts.\",\n",
        "    \"What challenges did Harry face in the Chamber of Secrets?\",\n",
        "    \"Explain the concept of Horcruxes and how they were used by Voldemort.\",\n",
        "    \"Describe a lesson with Professor Snape in Potions class.\",\n",
        "    \"Discuss the role of Dumbledore as headmaster of Hogwarts.\",\n",
        "    \"Explain the history and significance of the Deathly Hallows.\",\n",
        "    \"Describe a feast in the Great Hall at Hogwarts.\",\n",
        "    \"Discuss the rivalry between Gryffindor and Slytherin houses.\",\n",
        "    \"Describe a magical object that plays a key role in the Harry Potter series.\",\n",
        "    \"Explain the process of brewing Polyjuice Potion.\",\n",
        "    \"Discuss the significance of Harry's Patronus.\",\n",
        "    \"Describe a visit to Hogsmeade village.\",\n",
        "    \"Explain the concept of Animagi and how they are registered.\",\n",
        "    \"Discuss the role of house-elves in the Wizarding World.\",\n",
        "    \"Describe a visit to the Forbidden Forest at Hogwarts.\",\n",
        "    \"Explain how the Marauder's Map works.\",\n",
        "    \"Discuss the history and significance of the Mirror of Erised.\",\n",
        "    \"Describe a magical creature from the Harry Potter series and its characteristics.\",\n",
        "    \"Explain the rules and regulations of the Hogwarts Express train.\",\n",
        "    \"Discuss the importance of wandlore in the Wizarding World.\",\n",
        "    \"Describe a magical plant found in the Hogwarts greenhouse.\",\n",
        "    \"Explain the significance of the Sorting Hat ceremony at Hogwarts.\",\n",
        "    \"Discuss the role of the Ministry of Magic in regulating the Wizarding World.\",\n",
        "    \"Describe a magical duel between two characters in the Harry Potter series.\",\n",
        "    \"Explain the role of the Triwizard Cup in the Triwizard Tournament.\",\n",
        "    \"Discuss the significance of the Yule Ball in the fourth Harry Potter book.\",\n",
        "    \"Describe a magical object from the Harry Potter series that has the power to transport individuals.\",\n",
        "    \"Explain the process of creating a Horcrux and why it is considered dark magic.\",\n",
        "    \"Discuss the importance of Quidditch in the Wizarding World.\",\n",
        "    \"Describe a magical creature that is considered dangerous in the Harry Potter series.\",\n",
        "    \"Explain the concept of Occlumency and why it is important for wizards to learn.\",\n",
        "    \"Discuss the role of the Hogwarts Express train in transporting students to Hogwarts.\",\n",
        "    \"Describe a magical artifact that plays a key role in one of the Harry Potter books.\",\n",
        "    \"Explain the concept of the Triwizard Tournament and how it is organized.\",\n",
        "    \"Discuss the importance of the Room of Requirement in the Harry Potter series.\",\n",
        "    \"Describe a magical spell that is used for defensive purposes in the Wizarding World.\",\n",
        "    \"Explain the significance of the prophecy regarding Harry and Voldemort.\",\n",
        "    \"Discuss the role of house points in determining the House Cup winner at Hogwarts.\",\n",
        "    \"Describe a magical creature that is considered to be mythical in the Harry Potter series.\",\n",
        "    \"Explain the concept of the Unbreakable Vow and its consequences.\",\n",
        "    \"Discuss the role of the Hogwarts ghosts in the Harry Potter series.\",\n",
        "    \"Describe a magical object that has the power to manipulate time.\",\n",
        "    \"Explain the significance of Harry's Invisibility Cloak in the Harry Potter series.\"\n",
        "]\n",
        "\n",
        "# Move the entire model to the same device as the tokens\n",
        "model.to(device)\n",
        "\n",
        "# Generate text for each prompt\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"Generating text for prompt {i + 1}\")\n",
        "\n",
        "    # Tokenize the starting prompt\n",
        "    tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
        "    max_tokens = 2000\n",
        "    word_count = 0\n",
        "\n",
        "    # Generate up to max_tokens words\n",
        "    for _ in range(max_tokens):\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(tokens, max_new_tokens=1)\n",
        "        new_token = output[:, -1].item()\n",
        "        new_char = itos[new_token]\n",
        "        if new_char == '':\n",
        "            word_count += 1\n",
        "        new_tensor = torch.tensor([[new_token]], device=device)\n",
        "        tokens = torch.cat((tokens, new_tensor), dim=1)\n",
        "\n",
        "        if word_count >= 15000:\n",
        "            break\n",
        "\n",
        "    # Convert the token indices to characters and join them into a string\n",
        "    generated_text = ''.join([itos[idx] for idx in tokens.squeeze().tolist()])\n",
        "    print(generated_text)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nulzoqIDZed2",
        "outputId": "49528dca-da7f-4e1d-8d71-7188d0faeefe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating text for prompt 1\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "# List of 50 prompts\n",
        "prompts = [\n",
        "    \"What is the difference between Harry and Hagrid?\",\n",
        "    \"How did Voldemort lose his powers?\",\n",
        "    \"Why did Dumbledore trust Snape?\",\n",
        "    # Add your remaining prompts here\n",
        "]\n",
        "\n",
        "# Move the entire model to the same device as the tokens\n",
        "model.to(device)\n",
        "\n",
        "# Generate the conversation for each prompt\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"Generating text for prompt {i + 1}\")\n",
        "    tokens = torch.tensor(encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
        "    max_tokens = 2000\n",
        "    word_count = 0\n",
        "    while True:\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(tokens, max_new_tokens=1)\n",
        "        new_token = output[:, -1].item()\n",
        "        new_char = itos[new_token]\n",
        "        if new_char == '':\n",
        "            word_count += 1\n",
        "        new_tensor = torch.tensor([[new_token]], device=device)\n",
        "        tokens = torch.cat((tokens, new_tensor), dim=1)\n",
        "\n",
        "        if word_count >= 2000:\n",
        "            break\n",
        "\n",
        "    generated_text = ''.join([itos[idx] for idx in tokens.squeeze().tolist()])\n",
        "    print(generated_text)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1C7VquWlbZs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ryCd8P26lmHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f62259d-73c4-46cd-e939-262cb1a63200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text saved to file.\n"
          ]
        }
      ],
      "source": [
        "# Path to save the file\n",
        "file_path = '/content/drive/My Drive/saved_models/generated_text_l2.txt'\n",
        "\n",
        "# Write text to file\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(generated_text)\n",
        "\n",
        "print(\"Text saved to file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqxoKXLJwx2G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK9RtdG0yMvc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjN3JTdnyQJN"
      },
      "source": [
        "#17. Analysing Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6WL0cK4yU8o",
        "outputId": "52c386f4-0f17-45e2-ab9d-39fd0fe03713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of matching words: 131\n",
            "Accuracy: 0.5622317596566524\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import hashlib\n",
        "import json\n",
        "\n",
        "# Function to tokenize text into words\n",
        "def tokenize(text):\n",
        "    return set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
        "\n",
        "# Load the contents of both files\n",
        "file1_path = r'/content/drive/MyDrive/saved_models/sorcerers-stone.txt'\n",
        "file2_path = r'/content/drive/MyDrive/saved_models/generated_text_l2.txt'\n",
        "\n",
        "with open(file1_path, 'r', encoding='utf-8') as file1:\n",
        "    text1 = file1.read()\n",
        "\n",
        "with open(file2_path, 'r', encoding='utf-8') as file2:\n",
        "    text2 = file2.read()\n",
        "\n",
        "# Tokenize the text into words\n",
        "words_file1 = tokenize(text1)\n",
        "words_file2 = tokenize(text2)\n",
        "\n",
        "# Calculate the number of words in file 2 that are also in file 1\n",
        "matching_words = len(words_file2.intersection(words_file1))\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = matching_words / len(words_file2) if len(words_file2) > 0 else 0\n",
        "\n",
        "print(f\"Number of matching words: {matching_words}\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Save the dictionaries to files\n",
        "dict1 = {word: 1 for word in words_file1}\n",
        "dict2 = {word: 1 for word in words_file2}\n",
        "\n",
        "dict1_path = '/content/drive/MyDrive/saved_models/dict1.json'\n",
        "dict2_path = '/content/drive/MyDrive/saved_models/dict2.json'\n",
        "\n",
        "with open(dict1_path, 'w') as dict1_file:\n",
        "    json.dump(dict1, dict1_file, indent=4)\n",
        "\n",
        "with open(dict2_path, 'w') as dict2_file:\n",
        "    json.dump(dict2, dict2_file, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFuOz8GE-AAO"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nOTT9r3UqXHb",
        "dA9I5yBdPGnT",
        "paPGTnoZP0SV",
        "CGQAegsZQH4e",
        "EWJ0Z1qpQllR",
        "2HrBq6TYfhNO",
        "9Il3MOkxh9Gu",
        "tPb9rcNVRktD",
        "zc7x_2ZOSwds",
        "HCeAMYnKS7iF",
        "7Wgr06ZLTNvP",
        "evGWJ5IQTNfh",
        "5kFVjkoXUWct",
        "vr0327NQqj_l"
      ],
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1-8d6hucTibNQrLvxmQFiaRZ8ek9N9k4n",
      "authorship_tag": "ABX9TyM/IaqqLMRFdbELkkTiKK6l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}